{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install all required packages"
      ],
      "metadata": {
        "id": "RFVCB9fhXS-0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDtRfpvE1kic",
        "outputId": "aaa6760f-5552-4bda-c0ed-14ef1603fc1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-exporter-otlp-proto-common==1.38.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-proto==1.38.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-sdk~=1.38.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install chromadb sentence-transformers azure-ai-inference openpyxl pandas -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add your GITHUB PAT TOKEN to the secrets in google colab and then run this cell"
      ],
      "metadata": {
        "id": "QbESgVHIXZYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')"
      ],
      "metadata": {
        "id": "AvIYtuJi8HSH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import all the necessary libraries and read the Excel file."
      ],
      "metadata": {
        "id": "61YAJwRPXnpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "# path to your uploaded excel file (upload via the Colab file panel on the left)\n",
        "EXCEL_FILE_PATH = \"universities.xlsx\"  # change filename if yours is different"
      ],
      "metadata": {
        "id": "QQ3w6K0t7xiB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the excel file\n",
        "df = pd.read_excel(EXCEL_FILE_PATH)\n",
        "\n",
        "# fill blank/NaN values so they dont cause issues\n",
        "df = df.fillna(\"Not specified\")\n",
        "\n",
        "# print shape and columns so you can verify it loaded correctly\n",
        "print(f\"Loaded {len(df)} rows and {len(df.columns)} columns\")\n",
        "print(\"Columns:\", list(df.columns))\n",
        "print(\"\\nSample row:\\n\", df.iloc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcZjSGJ-8Q6Z",
        "outputId": "a4d4f92e-373c-4b80-e1ef-cf0f51221637"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 30 rows and 21 columns\n",
            "Columns: ['         University Name', '          City', '               Course', '     Deadline', '     Ilets', 'GPA german', ' credits', 'website link daad', '    vpd', 'Application fee', ' Specific requirement', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18', 'Unnamed: 19', 'Unnamed: 20']\n",
            "\n",
            "Sample row:\n",
            "          University Name                                  University of trier\n",
            "          City                                                          trier\n",
            "               Course                             Natural language processing\n",
            "     Deadline                                           6th May - 31 may 2025\n",
            "     Ilets                                                                6.5\n",
            "GPA german                                                      Not specified\n",
            " credits                                                        Not specified\n",
            "website link daad           https://www2.daad.de/deutschland/studienangebo...\n",
            "    vpd                                                          not required\n",
            "Application fee                                               NO (uni-portal)\n",
            " Specific requirement       a suitable Bachelor's degree (acquisition of a...\n",
            "Unnamed: 11                                                     Not specified\n",
            "Unnamed: 12                                                     Not specified\n",
            "Unnamed: 13                                                     Not specified\n",
            "Unnamed: 14                                                     Not specified\n",
            "Unnamed: 15                                                     Not specified\n",
            "Unnamed: 16                                                     Not specified\n",
            "Unnamed: 17                                                     Not specified\n",
            "Unnamed: 18                                                     Not specified\n",
            "Unnamed: 19                                                     Not specified\n",
            "Unnamed: 20                                                     Not specified\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To create the Vector DB convert each row to readable text and load the embedding model to generate embeddings for all rows."
      ],
      "metadata": {
        "id": "fhkn8F6CYH68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def row_to_text(row):\n",
        "    # converts a single dataframe row into a readable text string\n",
        "    # this becomes one \"chunk\" in the vector DB\n",
        "    parts = []\n",
        "    for col, val in row.items():\n",
        "        parts.append(f\"{col}: {val}\")\n",
        "    return \" | \".join(parts)\n",
        "\n",
        "# convert every row into a text chunk\n",
        "chunks = []\n",
        "chunk_ids = []\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    text = row_to_text(row)\n",
        "    chunks.append(text)\n",
        "    chunk_ids.append(str(i))\n",
        "\n",
        "print(f\"\\nTotal chunks created: {len(chunks)}\")\n",
        "print(\"\\nExample chunk:\\n\", chunks[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiSNtDBp8TsY",
        "outputId": "ecb09c78-376e-4b12-8c50-4d479cd46218"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total chunks created: 30\n",
            "\n",
            "Example chunk:\n",
            "          University Name: University of trier |           City: trier |                Course: Natural language processing |      Deadline: 6th May - 31 may 2025 |      Ilets: 6.5 | GPA german: Not specified |  credits: Not specified | website link daad: https://www2.daad.de/deutschland/studienangebote/international-programmes/en/detail/7708/ |     vpd: not required | Application fee: NO (uni-portal) |  Specific requirement: a suitable Bachelor's degree (acquisition of at least 180 ECTS from a domestic or foreign university) | Unnamed: 11: Not specified | Unnamed: 12: Not specified | Unnamed: 13: Not specified | Unnamed: 14: Not specified | Unnamed: 15: Not specified | Unnamed: 16: Not specified | Unnamed: 17: Not specified | Unnamed: 18: Not specified | Unnamed: 19: Not specified | Unnamed: 20: Not specified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading a lightweight local embedding model\n",
        "# this runs entirely on the colab machine, no API key needed\n",
        "print(\"Loading embedding model...\")\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# create embeddings for all chunks\n",
        "print(\"Generating embeddings for all university rows...\")\n",
        "embeddings = embedder.encode(chunks, show_progress_bar=True)\n",
        "\n",
        "# set up chromadb in-memory\n",
        "chroma_client = chromadb.Client()\n",
        "\n",
        "# create a collection (like a table in a DB)\n",
        "collection = chroma_client.create_collection(name=\"universities\")\n",
        "\n",
        "# add all chunks with their embeddings into the collection\n",
        "collection.add(\n",
        "    documents=chunks,\n",
        "    embeddings=embeddings.tolist(),\n",
        "    ids=chunk_ids\n",
        ")\n",
        "\n",
        "print(f\"\\nVector DB ready. Total documents indexed: {collection.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783,
          "referenced_widgets": [
            "3e3cc604bdca4bf8b783fb6850298c74",
            "ee5074dc1f274ff3ae703beab5259cfa",
            "0971a3239ddc4f8493c56fa0d1e455b4",
            "bbde49693b8245408298f944d972cebd",
            "48d079868aff4211b02d05d6c2e12379",
            "4f519b85aa5547fda460a4a10dd86f11",
            "35dd8de0fe13442389724ed86c1da369",
            "8d4aa873964c442ba44339e872667596",
            "b1c8aeaebe714faaa418bc2c80cf30e4",
            "84469baeba3e4df6859a534f86d3c06b",
            "b669037850914025a222611326a9b65e",
            "ed5416be7c064656bfba9c36ee6e4056",
            "d88d5454e9884f45a4e37ebddaf9aa31",
            "dff5e0bbf1284cd188558cc6e584c69e",
            "f756a564c5634e139319b2a56e004683",
            "dd57fca0b4c940d9b088aca97d31c6ce",
            "48a269a62bf749e9b87870400bf4da1d",
            "7d63c50c97664e3f8234e14e4b59535f",
            "9f91bcf8de9145f284853ac34225053c",
            "9143d7dbb00242678590b755a17c2e8f",
            "e050e4dcd10c49b9a61f8a5e2ecb08c3",
            "c24a5644edf043c7834b8e669d89e3cb",
            "24498a604f6c42eebaf16b425aefdf60",
            "8d625bc2fb694dd88e46258442915c21",
            "f7b98ec357ba46e2893973f845851c9b",
            "e5991472342045bcae4faa8cd70174bf",
            "d805233dc76744388204489472b12e46",
            "b349364a1aa7470ca631f23b1d85a157",
            "4c26b0d2291e421b9a1250bee58cccb7",
            "1ec84bd6199c43f5b8d82eda59258752",
            "d0824a385a1b449e892e0771840e5cf9",
            "8674fdf34f024ecfbc95f18ebaff06a2",
            "7e6f7eb6bbe24bb3830c0815ed559d5f",
            "95df6edd4c2a421eaa1ff03b6b391e20",
            "416c61523be249ebb28564977b0b96b8",
            "51a932a81ed7494585682fd1c54108c3",
            "396559f6c02f4bb0ad5a223b2a0f4273",
            "6093558bc19a42b8bf5af1a0281ba113",
            "d6ce87c2bc6049679e72f74d704a2a03",
            "8eda74ab601243ad8e57d6e3675d6b72",
            "bfb5e05bfcb64030a1adb8df78cf3e1b",
            "2c00b291679f44f8be2de77210be0f89",
            "4c1bea8996674093a17a5039198a5115",
            "d3c45c8d50c64e82a4823ec3ac7208eb",
            "3d86a7270f2041799c7bd330336a2631",
            "2e94ebc2987e4b0aa4bfe55182a87986",
            "e82529be0251422281ed8f557a7782e9",
            "0f67af39318d414eb87412e15c4d6977",
            "da4a90abce514755a0799eb24622a177",
            "43299b90486c4daba5ed499812286f90",
            "28d41094e9084711bb378f0da6f5bd2a",
            "389b9023b292413191ac83d4cae3f817",
            "5511ed03fff84e079509788dd55c6acd",
            "53e4fe493e7a4ae1897eb61514953c71",
            "ff0311190cf94a4b8a6424ea051f6f12",
            "21732fe1cd184119af6dd33b01b51444",
            "dfe02a159cdc433ebe863709341e09fd",
            "5facd6337fe241d19658ec7a88e7a69f",
            "5cf72c461f1a4cd19a9cf0cb3eb9995f",
            "6a8c904f7a0148dbbe995aced5b35a42",
            "a88895981bc347f38df5743ff592b79c",
            "1d445ef2c1a1411c92fc9f2103e98fcd",
            "e28b8f272fdc4071b20d644f4b2f0078",
            "8bc4395120e14b6ca94fe849f212ea7a",
            "99937b6232934ffd893d55908d390f07",
            "e538edeeb368403bb35adbf662b37f07",
            "ae28d2087cef49ae8c4c39e2e90a0248",
            "a41c39104fd44da59b6024ad8290f028",
            "b7f53a9a8c944084bcb64318b03b8ec3",
            "3366ad30f032449494a606f8f5586691",
            "d218988da3fa44d2b92dfc289454cb6b",
            "03db604cf7604862888b199896afd036",
            "04a46b7fb42b48579675afbd9efe723f",
            "b25eaa517fea45249338e0612fed3d01",
            "ad57134943a64a4fa003c774a2f7124b",
            "3726cac936d348da8d23c842a33d802d",
            "700ff21a2d654c28afd480032296c5c5",
            "c9feae77f3fa453cabfe4041695760a2",
            "39fb48a5013e450486bd21a04060db95",
            "f2b08e6c0aa643f3acb6e8813571888e",
            "3479208d6ce542e9b3b667a57c66550a",
            "b0ee8acc1bf84400a4ed5c332299eebe",
            "f58fa64f02a6476bb3d9e7f73d9d40e4",
            "105907b7018f4220913cce92e78614c3",
            "b763293a81f9455d839ea891c7453ae0",
            "c5e20a5fb78c48a789f3d5fcb92cedc0",
            "9ff277b10cf245f9b7126d0007194288",
            "012e7acb5da346f5b3aa4bf85e58eb7e",
            "98116f1aec3d449aa5b6ffe613b745d7",
            "947750ab95944f809cea8eddf55b8ca3",
            "33eeeccf48d04cefb0d0ac027dc16e40",
            "9ad5e1bc1d664f36af20fcd706566660",
            "10020961d91e4a56986100747ddf7495",
            "a5d49eb7c7c745669ed000c4f2faebb3",
            "25b00aeaeb684055813baee89481552f",
            "f9abbb27297e4d7a8de6521298b5cee6",
            "fa5a4c65848c4632b2b7ac9dfe622537",
            "e9bb9146e8e34cb7ae3bcb3e18f45236",
            "7ef20caa93b14aa5b2f66427be298726",
            "ae29517e0a4941219a5f4ea2407947fb",
            "6131d914b95d42498fc3e6a39614d00a",
            "b46e1ba1b56944798dd21df668d46824",
            "286f5aec70d041ff86dea45d31316f38",
            "208a7c069fc14c3b83034c31ecd00279",
            "4d39cb03a8ef46c7909088e4da9e486e",
            "f406ceafd28d4398a7c5a85ad4961806",
            "bc68e51aa0564b0991d2dd93ef7eef2c",
            "a3680339a5054ebda97a6b969b2a6781",
            "0ca8adfc6cff4e8693caf1feb6e6bea8",
            "157472f42a4c45e1a122c11391d1393a",
            "9b7f06d627074701acec77ade197ef46",
            "9fe08d35e30243e996433cc8ccffd666",
            "ebd4240a9d8f4d719e7af8dd932bc749",
            "5cdb9512271244eda46908ca46690ee2",
            "9422511b21224b8e8a6aae789e107a59",
            "bae4f3ac209647bfa15db1c1c36136b8",
            "8e1ba82f243d44c58a23aa5c4339d484",
            "0da9b401c4194129a75f22d6a68f8465",
            "c6ea4742cbe3435ead44cc127f8fa875",
            "b7f7b25bfcc64f99a6cc037311db971b",
            "2117a2060b7c49799f4ae672653eab6b",
            "5e9edc83658e4e33a4efc1743f7d3903",
            "c8af3d2b83314bd99a7fa11fe0821015",
            "2c03a0f5dd5b40619c79f9d2ef9176ba",
            "abe1dfee380e426faa5298d29b57cbf7",
            "75b2d0167cdc464092ace167c78a3fda",
            "34cb471c79154290a43042d547f11807",
            "f5871c7a5bac4cdca685d04235312df0",
            "6a66446311724b63a3084f632c99ebc8",
            "dd8bbea5f55345afa11dae06f9d72cd1",
            "c00e11c7aa87422a8e75496857b4ad2a",
            "30315b16639940da8ad99788b639b1b7",
            "146125da2fb145c2b3c33424231db408",
            "6b552180a98d4e11ae3cd37772f1a42b",
            "18ae04b23885472aa03071f7eeeb0ae7",
            "a93918445b26473ead600dee3456b9b4",
            "cb0264b30e85443f95ba49b9f74ca661",
            "150c6e9e133d408e9d5eb52597a13569",
            "dc15ece284384cf18db007af6d7d2dc2",
            "b2fa2f24d6004247b2f0507e007deb77",
            "829eeee5c7914852971cb1ac32a27625",
            "8ef0589aba9548768399e5aed45fc62c",
            "9ee62e8b2f824abca43edab765285a3e"
          ]
        },
        "id": "9lT_AFM08WQg",
        "outputId": "60fe7ae3-c114-4c12-c186-e9ff7a04f6a4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e3cc604bdca4bf8b783fb6850298c74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed5416be7c064656bfba9c36ee6e4056"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24498a604f6c42eebaf16b425aefdf60"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95df6edd4c2a421eaa1ff03b6b391e20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d86a7270f2041799c7bd330336a2631"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21732fe1cd184119af6dd33b01b51444"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae28d2087cef49ae8c4c39e2e90a0248"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9feae77f3fa453cabfe4041695760a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98116f1aec3d449aa5b6ffe613b745d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae29517e0a4941219a5f4ea2407947fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b7f06d627074701acec77ade197ef46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e9edc83658e4e33a4efc1743f7d3903"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating embeddings for all university rows...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "146125da2fb145c2b3c33424231db408"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vector DB ready. Total documents indexed: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the github marketplace LLM client"
      ],
      "metadata": {
        "id": "zWEjRI_ZYgiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_client = ChatCompletionsClient(\n",
        "    endpoint=\"https://models.github.ai/inference\",\n",
        "    credential=AzureKeyCredential(GITHUB_TOKEN),\n",
        ")"
      ],
      "metadata": {
        "id": "H_eVHQGB8a_Q"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keyword search scans every chunk for exact word matches from the query and ranks them by match count."
      ],
      "metadata": {
        "id": "ARTDk9ida3sS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def keyword_search(query, top_k=4):\n",
        "    # simple keyword search directly on the original dataframe\n",
        "    # splits query into individual words and checks if any row contains them\n",
        "    query_words = [w.lower().strip() for w in query.split() if len(w) > 2]\n",
        "\n",
        "    scores = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunk_lower = chunk.lower()\n",
        "        # count how many query words appear in this chunk\n",
        "        match_count = sum(1 for word in query_words if word in chunk_lower)\n",
        "        if match_count > 0:\n",
        "            scores.append((i, match_count))\n",
        "\n",
        "    # sort by match count descending\n",
        "    scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    top_indices = [str(idx) for idx, _ in scores[:top_k]]\n",
        "    top_chunks = [chunks[idx] for idx, _ in scores[:top_k]]\n",
        "    return top_indices, top_chunks"
      ],
      "metadata": {
        "id": "li2pYsV0ai3b"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid retrieval combines semantic search from ChromaDB and keyword search, merges both results and deduplicates them."
      ],
      "metadata": {
        "id": "ULQVpigHa8bE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_hybrid(query, top_k=4, verbose=False):\n",
        "    # step 1: semantic search via chromadb\n",
        "    query_embedding = embedder.encode([query]).tolist()\n",
        "    semantic_results = collection.query(\n",
        "        query_embeddings=query_embedding,\n",
        "        n_results=top_k\n",
        "    )\n",
        "    semantic_ids = semantic_results[\"ids\"][0]\n",
        "    semantic_chunks = semantic_results[\"documents\"][0]\n",
        "\n",
        "    # step 2: keyword search on raw chunks\n",
        "    keyword_ids, keyword_chunks = keyword_search(query, top_k=top_k)\n",
        "\n",
        "    # step 3: merge both result sets, deduplicate, preserve order\n",
        "    # keyword results go first so they dont get buried if semantics failed\n",
        "    seen_ids = set()\n",
        "    merged_chunks = []\n",
        "\n",
        "    for id_, chunk in zip(keyword_ids, keyword_chunks):\n",
        "        if id_ not in seen_ids:\n",
        "            seen_ids.add(id_)\n",
        "            merged_chunks.append((\"keyword\", chunk))\n",
        "\n",
        "    for id_, chunk in zip(semantic_ids, semantic_chunks):\n",
        "        if id_ not in seen_ids:\n",
        "            seen_ids.add(id_)\n",
        "            merged_chunks.append((\"semantic\", chunk))\n",
        "\n",
        "    if verbose:\n",
        "        print(\"--- HYBRID RETRIEVAL RESULTS ---\")\n",
        "        for source, chunk in merged_chunks:\n",
        "            print(f\"[{source}] {chunk}\\n\")\n",
        "        print(\"--------------------------------\\n\")\n",
        "\n",
        "    # return just the chunks for the LLM\n",
        "    return [chunk for _, chunk in merged_chunks]"
      ],
      "metadata": {
        "id": "lB78Y0w5apTD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ask_rag ties everything together, builds the prompt with retrieved context and calls the LLM to get the final answer."
      ],
      "metadata": {
        "id": "Yqtf87aibCeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_rag(question, top_k=4, verbose=False):\n",
        "    # uses hybrid retrieval instead of pure semantic\n",
        "    relevant_chunks = retrieve_hybrid(question, top_k=top_k, verbose=verbose)\n",
        "\n",
        "    context = \"\\n\\n\".join([f\"Entry {i+1}: {chunk}\" for i, chunk in enumerate(relevant_chunks)])\n",
        "\n",
        "    prompt = f\"\"\"You are a helpful assistant that answers questions about German university admissions.\n",
        "Use ONLY the information provided below to answer the question. If the answer is not in the context, say \"I don't have that information in the dataset.\"\n",
        "\n",
        "Context from the university dataset:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    response = llm_client.complete(\n",
        "        messages=[\n",
        "            SystemMessage(\"You are a helpful assistant for German university admissions. Answer based only on the provided context.\"),\n",
        "            UserMessage(prompt),\n",
        "        ],\n",
        "        model=\"meta/Llama-3.3-70B-Instruct\",\n",
        "        temperature=0.3,\n",
        "        max_tokens=600,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "RRFSt_IKatQj"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alas Testing the RAG with question."
      ],
      "metadata": {
        "id": "jFGDL-38ZfEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change the question here and just re-run this single cell\n",
        "# set verbose=True if you want to see which university rows were retrieved\n",
        "question = \"which universities accept IELTS 6.5 for artificial intelligence and have deadlines after April\"\n",
        "answer = ask_rag(question, top_k=4)\n",
        "\n",
        "print(\"Question:\", question)\n",
        "print(\"\\nAnswer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-pOrxOr8dqo",
        "outputId": "916e2e11-334f-4d85-cd75-60110f99a1d5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: which universities accept IELTS 6.5 for artificial intelligence and have deadlines after April\n",
            "\n",
            "Answer: Based on the provided context, the universities that accept IELTS 6.5 for Artificial Intelligence and have deadlines after April are:\n",
            "\n",
            "1. Hof University of Applied Science (Deadline: 15 April - 30 May 2025) - Course: Artificial Intelligence and Robotics M.Sc.\n",
            "2. University of Regensburg (Deadline: 15 April - 1 June 2025) - Course: Human-Centred Artificial Intelligence\n",
            "3. Deggendorf Institute of Technology (Deadline: Not specified, but has a course in Artificial Intelligence) - Course: MSc Artificial Intelligence and Data Science\n",
            "4. Hochschule Neu ULM University (Deadline: 2 May - 15 July 2025) - Course: Artificial intelligence and data analytics \n",
            "\n",
            "Note that Entry 2 (University of Passau) also has a course related to Artificial Intelligence (Artificial intelligence engineering), but its deadline is 1 April- 31 May 2025, which starts before April, so it's not included in the list. Entry 5 (Deggendorf Institute of Technology) has a deadline that is not specified, so it's included in the list with a note.\n"
          ]
        }
      ]
    }
  ]
}
